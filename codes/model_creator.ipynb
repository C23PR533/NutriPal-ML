{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 kebab', 'calories': '620', 'carbohydrates': '77.10', 'fat': '16.63', 'protein': '37.87'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 serving (390 g)', 'calories': '620', 'carbohydrates': '77.10', 'fat': '16.63', 'protein': '37.87'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '100 g', 'calories': '159', 'carbohydrates': '19.77', 'fat': '4.27', 'protein': '9.71'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 oz', 'calories': '45', 'carbohydrates': '5.60', 'fat': '1.21', 'protein': '2.75'}\n",
      "{'food_id': '58704864', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '4 gyoza', 'calories': '120', 'carbohydrates': '17.00', 'fat': '2.50', 'protein': '7.00'}\n",
      "{'food_id': '68848666', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '5 pieces', 'calories': '250', 'carbohydrates': '30.00', 'fat': '11.00', 'protein': '7.00'}\n",
      "{'food_id': '515347', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '130', 'carbohydrates': '17.00', 'fat': '4.50', 'protein': '5.50'}\n",
      "{'food_id': '1684073', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '350', 'carbohydrates': '12.00', 'fat': '24.00', 'protein': '22.00'}\n",
      "{'food_id': '360970', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '290', 'carbohydrates': '3.00', 'fat': '16.00', 'protein': '30.00'}\n",
      "{'food_id': '51780578', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '260', 'carbohydrates': '31.00', 'fat': '5.00', 'protein': '21.00'}\n",
      "{'food_id': '35362184', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '710', 'carbohydrates': '67.00', 'fat': '32.00', 'protein': '35.00'}\n",
      "{'food_id': '1751238', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '890', 'carbohydrates': '55.00', 'fat': '58.00', 'protein': '38.00'}\n",
      "{'food_id': '59433475', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 chicken', 'calories': '1320', 'carbohydrates': '115.00', 'fat': '67.00', 'protein': '65.00'}\n",
      "{'food_id': '3451226', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '270', 'carbohydrates': '16.00', 'fat': '13.00', 'protein': '21.00'}\n",
      "{'food_id': '1565707', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '360', 'carbohydrates': '9.00', 'fat': '13.00', 'protein': '48.00'}\n",
      "{'food_id': '855710', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '240', 'carbohydrates': '10.00', 'fat': '11.00', 'protein': '25.00'}\n",
      "{'food_id': '6926210', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '5 oz', 'calories': '240', 'carbohydrates': '5.00', 'fat': '14.00', 'protein': '23.00'}\n",
      "{'food_id': '50672307', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 tray', 'calories': '430', 'carbohydrates': '11.00', 'fat': '24.00', 'protein': '43.00'}\n",
      "{'food_id': '226469', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 breast', 'calories': '580', 'carbohydrates': '21.00', 'fat': '25.00', 'protein': '68.00'}\n",
      "{'food_id': '54876480', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 tray', 'calories': '670', 'carbohydrates': '29.00', 'fat': '43.00', 'protein': '46.00'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load data from the JSON file\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# Preprocess the data\n",
    "\n",
    "food_items = []\n",
    "for item in data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    food_type = item['food']['food_type']\n",
    "    servings = item['food']['servings']['serving']\n",
    "\n",
    "    # Process each serving entry\n",
    "    for serving in servings:\n",
    "        serving_description = serving['serving_description']\n",
    "        calories = serving['calories']\n",
    "        carbohydrates = serving['carbohydrate']\n",
    "        fat = serving['fat']\n",
    "        protein = serving['protein']\n",
    "\n",
    "        # Add the processed data to the food_items list\n",
    "        food_items.append({\n",
    "            'food_id': food_id,\n",
    "            'food_name': food_name,\n",
    "            'food_type': food_type,\n",
    "            'serving_description': serving_description,\n",
    "            'calories': calories,\n",
    "            'carbohydrates': carbohydrates,\n",
    "            'fat': fat,\n",
    "            'protein': protein\n",
    "        })\n",
    "# Print the first few food items\n",
    "for item in food_items[:20]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError()]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Create an instance of the hybrid model\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHybridModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     84\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "Cell \u001b[1;32mIn[6], line 64\u001b[0m, in \u001b[0;36mHybridModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollaborative_model \u001b[38;5;241m=\u001b[39m CollaborativeFilteringModel()\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_based_model \u001b[38;5;241m=\u001b[39m \u001b[43mContentBasedModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 53\u001b[0m, in \u001b[0;36mContentBasedModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Define the layers for content-based filtering\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[43mitem_vocab_size\u001b[49m, embedding_dim)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item_vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Load data from the food.json file\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "    food_data = json.load(file)\n",
    "\n",
    "# Load user preferences from the user.json file\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset.json', 'r') as file:\n",
    "    user_data = json.load(file)\n",
    "\n",
    "# Assuming user preferences are stored in a list\n",
    "if isinstance(user_data, list):\n",
    "    user_preferences = user_data[0]['listUserPreferences']\n",
    "else:\n",
    "    user_preferences = user_data['listUserPreferences']\n",
    "\n",
    "# Preprocess the food data\n",
    "food_items = []\n",
    "for item in food_data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    # Add other relevant features from the food.json file to the food_items list\n",
    "\n",
    "# Preprocess the user preferences\n",
    "gender = user_preferences['gender']\n",
    "weight = float(user_preferences['weight'])\n",
    "# Add other relevant user preferences from the user.json file\n",
    "\n",
    "# Determine the vocabulary size based on your data\n",
    "user_vocab_size = 1000  # Replace with the actual vocabulary size\n",
    "\n",
    "# Define the desired dimensionality of the embedding space\n",
    "embedding_dim = 64  # Replace with your preferred value\n",
    "\n",
    "# Determine the vocabulary size for items based on your data\n",
    "item_vocab_size = 1000  # Replace with the actual vocabulary size\n",
    "\n",
    "# Define the collaborative filtering model\n",
    "class CollaborativeFilteringModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers for collaborative filtering\n",
    "        self.embedding = tf.keras.layers.Embedding(user_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.embedding(inputs['user_id'])\n",
    "        return user_embeddings\n",
    "\n",
    "# Define the content-based model\n",
    "class ContentBasedModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers for content-based filtering\n",
    "        self.embedding = tf.keras.layers.Embedding(item_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        item_embeddings = self.embedding(inputs['item_id'])\n",
    "        return item_embeddings\n",
    "\n",
    "# Define the hybrid model\n",
    "class HybridModel(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.collaborative_model = CollaborativeFilteringModel()\n",
    "        self.content_based_model = ContentBasedModel()\n",
    "        self.dense_layer = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.final_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.collaborative_model(inputs)\n",
    "        item_embeddings = self.content_based_model(inputs)\n",
    "        concatenated_embeddings = tf.concat([user_embeddings, item_embeddings], axis=1)\n",
    "        x = self.dense_layer(concatenated_embeddings)\n",
    "        output = self.final_layer(x)\n",
    "        return output\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Create an instance of the hybrid model\n",
    "model = HybridModel()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_data)\n",
    "print(f\"Test RMSE: {results[1]}\")\n",
    "\n",
    "# Make recommendations\n",
    "user_inputs = {'user_id': tf.constant([user_id])}  # User ID for which recommendations will be generated\n",
    "item_inputs = {'item_id': tf.constant(item_ids)}  # Item IDs to consider for recommendations\n",
    "\n",
    "recommendations = model.predict((user_inputs, item_inputs))\n",
    "print(recommendations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2db04e191085c925f35704d5645a1f5ec8c10b267e24c755d6661bdf1429d0eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
