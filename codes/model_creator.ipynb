{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 kebab', 'calories': '620', 'carbohydrates': '77.10', 'fat': '16.63', 'protein': '37.87'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 serving (390 g)', 'calories': '620', 'carbohydrates': '77.10', 'fat': '16.63', 'protein': '37.87'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '100 g', 'calories': '159', 'carbohydrates': '19.77', 'fat': '4.27', 'protein': '9.71'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 oz', 'calories': '45', 'carbohydrates': '5.60', 'fat': '1.21', 'protein': '2.75'}\n",
      "{'food_id': '58704864', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '4 gyoza', 'calories': '120', 'carbohydrates': '17.00', 'fat': '2.50', 'protein': '7.00'}\n",
      "{'food_id': '68848666', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '5 pieces', 'calories': '250', 'carbohydrates': '30.00', 'fat': '11.00', 'protein': '7.00'}\n",
      "{'food_id': '515347', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '130', 'carbohydrates': '17.00', 'fat': '4.50', 'protein': '5.50'}\n",
      "{'food_id': '1684073', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '350', 'carbohydrates': '12.00', 'fat': '24.00', 'protein': '22.00'}\n",
      "{'food_id': '360970', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '290', 'carbohydrates': '3.00', 'fat': '16.00', 'protein': '30.00'}\n",
      "{'food_id': '51780578', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '260', 'carbohydrates': '31.00', 'fat': '5.00', 'protein': '21.00'}\n",
      "{'food_id': '35362184', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '710', 'carbohydrates': '67.00', 'fat': '32.00', 'protein': '35.00'}\n",
      "{'food_id': '1751238', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '890', 'carbohydrates': '55.00', 'fat': '58.00', 'protein': '38.00'}\n",
      "{'food_id': '59433475', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 chicken', 'calories': '1320', 'carbohydrates': '115.00', 'fat': '67.00', 'protein': '65.00'}\n",
      "{'food_id': '3451226', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '270', 'carbohydrates': '16.00', 'fat': '13.00', 'protein': '21.00'}\n",
      "{'food_id': '1565707', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '360', 'carbohydrates': '9.00', 'fat': '13.00', 'protein': '48.00'}\n",
      "{'food_id': '855710', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '240', 'carbohydrates': '10.00', 'fat': '11.00', 'protein': '25.00'}\n",
      "{'food_id': '6926210', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '5 oz', 'calories': '240', 'carbohydrates': '5.00', 'fat': '14.00', 'protein': '23.00'}\n",
      "{'food_id': '50672307', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 tray', 'calories': '430', 'carbohydrates': '11.00', 'fat': '24.00', 'protein': '43.00'}\n",
      "{'food_id': '226469', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 breast', 'calories': '580', 'carbohydrates': '21.00', 'fat': '25.00', 'protein': '68.00'}\n",
      "{'food_id': '54876480', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 tray', 'calories': '670', 'carbohydrates': '29.00', 'fat': '43.00', 'protein': '46.00'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load data from the JSON file\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# Preprocess the data\n",
    "\n",
    "food_items = []\n",
    "for item in data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    food_type = item['food']['food_type']\n",
    "    servings = item['food']['servings']['serving']\n",
    "\n",
    "    # Process each serving entry\n",
    "    for serving in servings:\n",
    "        serving_description = serving['serving_description']\n",
    "        calories = serving['calories']\n",
    "        carbohydrates = serving['carbohydrate']\n",
    "        fat = serving['fat']\n",
    "        protein = serving['protein']\n",
    "\n",
    "        # Add the processed data to the food_items list\n",
    "        food_items.append({\n",
    "            'food_id': food_id,\n",
    "            'food_name': food_name,\n",
    "            'food_type': food_type,\n",
    "            'serving_description': serving_description,\n",
    "            'calories': calories,\n",
    "            'carbohydrates': carbohydrates,\n",
    "            'fat': fat,\n",
    "            'protein': protein\n",
    "        })\n",
    "# Print the first few food items\n",
    "for item in food_items[:20]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:103\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:487\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    484\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m    485\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    488\u001b[0m     element,\n\u001b[0;32m    489\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [Ellipsis] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m train_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((train_user_ids, train_item_ids, train_targets))\n\u001b[0;32m     68\u001b[0m val_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((val_user_ids, val_item_ids, val_targets))\n\u001b[1;32m---> 69\u001b[0m test_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices((test_user_ids, test_item_ids, test_targets))\n\u001b[0;32m     71\u001b[0m \u001b[39m# Shuffle and batch the datasets\u001b[39;00m\n\u001b[0;32m     72\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4708\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4706\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   4707\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4708\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[0;32m   4709\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4710\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:108\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 108\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, sparse_tensor\u001b[39m.\u001b[39mSparseTensorSpec):\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1629\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1631\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1635\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m   1637\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1638\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1641\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Load data from the food.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "        food_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Load user preferences from the user.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset.json', 'r') as file:\n",
    "        user_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Preprocess the food data\n",
    "food_items = []\n",
    "for item in food_data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    # Add other relevant features from the food.json file to the food_items list\n",
    "    food_items.append((food_id, food_name))\n",
    "\n",
    "# Preprocess the user preferences\n",
    "if isinstance(user_data, list):\n",
    "    user_preferences = user_data[0]['listUserPreferences']\n",
    "else:\n",
    "    user_preferences = user_data['listUserPreferences']\n",
    "\n",
    "gender = user_preferences['gender']\n",
    "weight = float(user_preferences['weight'])\n",
    "# Add other relevant user preferences from the user.json file\n",
    "\n",
    "# Define the variables\n",
    "user_vocab_size = 1000\n",
    "item_vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "\n",
    "# user_ids = [...]  # List of user IDs\n",
    "# Open the user_ids.json file and load its contents into the user_ids array\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_ids.json', 'r') as f:\n",
    "    user_ids = json.load(f)\n",
    "\n",
    "\n",
    "# item_ids = [...]  # List of item IDs\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/food_ids.json', 'r') as f:\n",
    "    food_id = json.load(f)\n",
    "\n",
    "\n",
    "# target_values = [...]  # List of target values\n",
    "target_values = []\n",
    "\n",
    "for user_id, item_id in zip(user_ids, food_id):\n",
    "    # Assuming user_data is a dictionary with user preferences\n",
    "    user_preferences = user_data[user_id]\n",
    "    # Function to get harmful diseases for a food item\n",
    "    harmful_diseases = get_harmful_diseases(item_id)\n",
    "\n",
    "    if any(disease in user_preferences['disease'] for disease in harmful_diseases):\n",
    "        # Assign lower target value for harmful foods\n",
    "        target_values.append(0.0)\n",
    "    else:\n",
    "        # Assign higher target value for preferred foods\n",
    "        target_values.append(1.0)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_size = int(0.7 * len(user_ids))\n",
    "val_size = int(0.15 * len(user_ids))\n",
    "test_size = len(user_ids) - train_size - val_size\n",
    "\n",
    "train_user_ids = user_ids[:train_size]\n",
    "train_item_ids = item_ids[:train_size]\n",
    "train_targets = target_values[:train_size]\n",
    "\n",
    "val_user_ids = user_ids[train_size:train_size+val_size]\n",
    "val_item_ids = item_ids[train_size:train_size+val_size]\n",
    "val_targets = target_values[train_size:train_size+val_size]\n",
    "\n",
    "test_user_ids = user_ids[train_size+val_size:]\n",
    "test_item_ids = item_ids[train_size+val_size:]\n",
    "test_targets = target_values[train_size+val_size:]\n",
    "\n",
    "# Convert the data to TensorFlow Datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_user_ids, train_item_ids, train_targets))\n",
    "val_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_user_ids, val_item_ids, val_targets))\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_user_ids, test_item_ids, test_targets))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "batch_size = 32\n",
    "train_data = train_data.shuffle(buffer_size=train_size).batch(batch_size)\n",
    "val_data = val_data.shuffle(buffer_size=val_size).batch(batch_size)\n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "# Define the collaborative filtering model\n",
    "\n",
    "\n",
    "class CollaborativeFilteringModel(tf.keras.Model):\n",
    "    def __init__(self, user_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Define the layers for collaborative filtering\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            user_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.embedding(inputs['user_id'])\n",
    "        return user_embeddings\n",
    "\n",
    "# Define the content-based model\n",
    "\n",
    "\n",
    "class ContentBasedModel(tf.keras.Model):\n",
    "    def __init__(self, item_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Define the layers for content-based filtering\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            item_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        item_embeddings = self.embedding(inputs['item_id'])\n",
    "        return item_embeddings\n",
    "\n",
    "# Define the hybrid model\n",
    "\n",
    "\n",
    "class HybridModel(tfrs.Model):\n",
    "    def __init__(self, user_vocab_size, item_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.collaborative_model = CollaborativeFilteringModel(\n",
    "            user_vocab_size, embedding_dim)\n",
    "        self.content_based_model = ContentBasedModel(\n",
    "            item_vocab_size, embedding_dim)\n",
    "        self.dense_layer = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.final_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.collaborative_model(inputs)\n",
    "        item_embeddings = self.content_based_model(inputs)\n",
    "        concatenated_embeddings = tf.concat(\n",
    "            [user_embeddings, item_embeddings], axis=1)\n",
    "        x = self.dense_layer(concatenated_embeddings)\n",
    "        output = self.final_layer(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Create an instance of the hybrid model\n",
    "model = HybridModel(user_vocab_size, item_vocab_size, embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001), loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_data)\n",
    "print(f\"Test RMSE: {results[1]}\")\n",
    "\n",
    "# Make recommendations\n",
    "# User ID for which recommendations will be generated\n",
    "user_inputs = {'user_id': tf.constant([user_ids])}\n",
    "# Item IDs to consider for recommendations\n",
    "item_inputs = {'item_id': tf.constant(item_ids)}\n",
    "recommendations = model.predict((user_inputs, item_inputs))\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the food.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "        food_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Load user preferences from the user.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset_new.json', 'r') as file:\n",
    "        user_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get harmful diseases for a given food item\n",
    "def get_harmful_diseases(food_id):\n",
    "    harmful_diseases = []\n",
    "    for item in food_data:\n",
    "        if item['food']['food_id'] == food_id:\n",
    "            harmful_diseases = item['food']['harmful_diseases']\n",
    "            break\n",
    "    return harmful_diseases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the food data\n",
    "food_items = []\n",
    "for item in food_data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    # Add other relevant features from the food.json file to the food_items list\n",
    "    food_items.append((food_id, food_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the user preferences\n",
    "if isinstance(user_data, list):\n",
    "    user_data = user_data[0]\n",
    "user_preferences = user_data['listUserPreferences']\n",
    "gender = user_preferences['gender']\n",
    "weight = float(user_preferences['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables\n",
    "user_vocab_size = 1000\n",
    "item_vocab_size = 1000\n",
    "embedding_dim = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids = [...]  # List of user IDs\n",
    "# Open the user_ids.json file and load its contents into the user_ids array\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_ids.json', 'r') as f:\n",
    "    user_ids = json.load(f)\n",
    "\n",
    "# item_ids = [...]  # List of item IDs\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/food_ids.json', 'r') as f:\n",
    "    item_ids = json.load(f)['food_ids']\n",
    "\n",
    "target_values = []  # List of target values\n",
    "\n",
    "# Fill target values based on user preferences and harmful diseases\n",
    "for user_id, item_id in zip(user_ids, item_ids):\n",
    "    user_preference = user_data.get(str(user_id))\n",
    "    if user_preference is None:\n",
    "        # User preferences not found, assign lower target value\n",
    "        target_values.append(0.0)\n",
    "    else:\n",
    "        user_preference = user_preference['listUserPreferences']\n",
    "        harmful_diseases = get_harmful_diseases(item_id)\n",
    "\n",
    "        if any(disease in user_preference['disease'] for disease in harmful_diseases):\n",
    "            # Assign lower target value for harmful foods\n",
    "            target_values.append(0.0)\n",
    "        else:\n",
    "            # Assign higher target value for preferred foods\n",
    "            target_values.append(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_size = int(0.7 * len(user_ids))\n",
    "val_size = int(0.15 * len(user_ids))\n",
    "test_size = len(user_ids) - train_size - val_size\n",
    "\n",
    "train_user_ids = user_ids[:train_size]\n",
    "train_item_ids = item_ids[:train_size]\n",
    "train_targets = target_values[:train_size]\n",
    "\n",
    "val_user_ids = user_ids[train_size:train_size+val_size]\n",
    "val_item_ids = item_ids[train_size:train_size+val_size]\n",
    "val_targets = target_values[train_size:train_size+val_size]\n",
    "\n",
    "test_user_ids = user_ids[train_size+val_size:]\n",
    "test_item_ids = item_ids[train_size+val_size:]\n",
    "test_targets = target_values[train_size+val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_user_ids size: 350\n",
      "train_item_ids size: 265\n",
      "train_targets size: 265\n",
      "val_user_ids size: 75\n",
      "val_item_ids size: 0\n",
      "val_targets size: 0\n",
      "test_user_ids size: 75\n",
      "test_item_ids size: 0\n",
      "test_targets size: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"train_user_ids size:\", len(train_user_ids))\n",
    "print(\"train_item_ids size:\", len(train_item_ids))\n",
    "print(\"train_targets size:\", len(train_targets))\n",
    "\n",
    "print(\"val_user_ids size:\", len(val_user_ids))\n",
    "print(\"val_item_ids size:\", len(val_item_ids))\n",
    "print(\"val_targets size:\", len(val_targets))\n",
    "\n",
    "print(\"test_user_ids size:\", len(test_user_ids))\n",
    "print(\"test_item_ids size:\", len(test_item_ids))\n",
    "print(\"test_targets size:\", len(test_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 265 and 350 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Convert the data to TensorFlow Datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices(\n\u001b[0;32m      3\u001b[0m     ({\u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_user_ids, \u001b[39m\"\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_item_ids}, train_targets)\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m val_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m      6\u001b[0m     ({\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: val_user_ids, \u001b[39m\"\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m\"\u001b[39m: val_item_ids}, val_targets)\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m test_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m      9\u001b[0m     ({\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: test_user_ids, \u001b[39m\"\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m\"\u001b[39m: test_item_ids}, test_targets)\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4720\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4717\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[0;32m   4718\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n\u001b[0;32m   4719\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m-> 4720\u001b[0m   batch_dim\u001b[39m.\u001b[39;49massert_is_compatible_with(\n\u001b[0;32m   4721\u001b[0m       tensor_shape\u001b[39m.\u001b[39;49mDimension(\n\u001b[0;32m   4722\u001b[0m           tensor_shape\u001b[39m.\u001b[39;49mdimension_value(t\u001b[39m.\u001b[39;49mget_shape()[\u001b[39m0\u001b[39;49m])))\n\u001b[0;32m   4724\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mtensor_slice_dataset(\n\u001b[0;32m   4725\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors,\n\u001b[0;32m   4726\u001b[0m     output_shapes\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_shapes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure),\n\u001b[0;32m   4727\u001b[0m     is_files\u001b[39m=\u001b[39mis_files,\n\u001b[0;32m   4728\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mSerializeToString())\n\u001b[0;32m   4729\u001b[0m \u001b[39msuper\u001b[39m(TensorSliceDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:299\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m    is_compatible_with).\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m--> 299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDimensions \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m are not compatible\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    300\u001b[0m                    (\u001b[39mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions 265 and 350 are not compatible"
     ]
    }
   ],
   "source": [
    "# Convert the data to TensorFlow Datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": train_user_ids, \"item_id\": train_item_ids}, train_targets)\n",
    ")\n",
    "val_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": val_user_ids, \"item_id\": val_item_ids}, val_targets)\n",
    ")\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": test_user_ids, \"item_id\": test_item_ids}, test_targets)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 265 and 350 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m test_targets \u001b[39m=\u001b[39m target_values[train_size\u001b[39m+\u001b[39mval_size:]\n\u001b[0;32m    101\u001b[0m \u001b[39m# Convert the data to TensorFlow Datasets\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m train_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices(\n\u001b[0;32m    103\u001b[0m     ({\u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_user_ids, \u001b[39m\"\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_item_ids}, train_targets)\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    105\u001b[0m val_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m    106\u001b[0m     ({\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: val_user_ids, \u001b[39m\"\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m\"\u001b[39m: val_item_ids}, val_targets)\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m test_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m    109\u001b[0m     ({\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: test_user_ids, \u001b[39m\"\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m\"\u001b[39m: test_item_ids}, test_targets)\n\u001b[0;32m    110\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4720\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4717\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[0;32m   4718\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n\u001b[0;32m   4719\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m-> 4720\u001b[0m   batch_dim\u001b[39m.\u001b[39;49massert_is_compatible_with(\n\u001b[0;32m   4721\u001b[0m       tensor_shape\u001b[39m.\u001b[39;49mDimension(\n\u001b[0;32m   4722\u001b[0m           tensor_shape\u001b[39m.\u001b[39;49mdimension_value(t\u001b[39m.\u001b[39;49mget_shape()[\u001b[39m0\u001b[39;49m])))\n\u001b[0;32m   4724\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mtensor_slice_dataset(\n\u001b[0;32m   4725\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors,\n\u001b[0;32m   4726\u001b[0m     output_shapes\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_shapes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure),\n\u001b[0;32m   4727\u001b[0m     is_files\u001b[39m=\u001b[39mis_files,\n\u001b[0;32m   4728\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mSerializeToString())\n\u001b[0;32m   4729\u001b[0m \u001b[39msuper\u001b[39m(TensorSliceDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:299\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m    is_compatible_with).\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m--> 299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDimensions \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m are not compatible\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    300\u001b[0m                    (\u001b[39mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions 265 and 350 are not compatible"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            user_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.user_embedding(inputs[\"user_id\"])\n",
    "\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.item_embedding = tf.keras.layers.Embedding(\n",
    "            item_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.item_embedding(inputs[\"item_id\"])\n",
    "\n",
    "\n",
    "class RecommenderModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_model = UserModel()\n",
    "        self.item_model = ItemModel()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features)\n",
    "        item_embeddings = self.item_model(features)\n",
    "        return self.task(user_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecommenderModel\n",
    "recommender_model = RecommenderModel()\n",
    "\n",
    "# Compile the model\n",
    "recommender_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Train the model\n",
    "recommender_model.fit(train_data.shuffle(1000).batch(32), epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = recommender_model.evaluate(val_data.batch(32), return_dict=True)\n",
    "print(f\"Validation RMSE: {eval_results['root_mean_squared_error']}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = recommender_model.predict(test_data.batch(32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2db04e191085c925f35704d5645a1f5ec8c10b267e24c755d6661bdf1429d0eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
