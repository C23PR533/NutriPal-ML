{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 kebab', 'calories': '620', 'carbohydrates': '77.10', 'fat': '16.63', 'protein': '37.87'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 serving (390 g)', 'calories': '620', 'carbohydrates': '77.10', 'fat': '16.63', 'protein': '37.87'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '100 g', 'calories': '159', 'carbohydrates': '19.77', 'fat': '4.27', 'protein': '9.71'}\n",
      "{'food_id': '2655309', 'food_name': 'Kebab', 'food_type': 'Generic', 'serving_description': '1 oz', 'calories': '45', 'carbohydrates': '5.60', 'fat': '1.21', 'protein': '2.75'}\n",
      "{'food_id': '58704864', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '4 gyoza', 'calories': '120', 'carbohydrates': '17.00', 'fat': '2.50', 'protein': '7.00'}\n",
      "{'food_id': '68848666', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '5 pieces', 'calories': '250', 'carbohydrates': '30.00', 'fat': '11.00', 'protein': '7.00'}\n",
      "{'food_id': '515347', 'food_name': 'Gyoza', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '130', 'carbohydrates': '17.00', 'fat': '4.50', 'protein': '5.50'}\n",
      "{'food_id': '1684073', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '350', 'carbohydrates': '12.00', 'fat': '24.00', 'protein': '22.00'}\n",
      "{'food_id': '360970', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '290', 'carbohydrates': '3.00', 'fat': '16.00', 'protein': '30.00'}\n",
      "{'food_id': '51780578', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '260', 'carbohydrates': '31.00', 'fat': '5.00', 'protein': '21.00'}\n",
      "{'food_id': '35362184', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '710', 'carbohydrates': '67.00', 'fat': '32.00', 'protein': '35.00'}\n",
      "{'food_id': '1751238', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 serving', 'calories': '890', 'carbohydrates': '55.00', 'fat': '58.00', 'protein': '38.00'}\n",
      "{'food_id': '59433475', 'food_name': 'Chicken Katsu', 'food_type': 'Brand', 'serving_description': '1 chicken', 'calories': '1320', 'carbohydrates': '115.00', 'fat': '67.00', 'protein': '65.00'}\n",
      "{'food_id': '3451226', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '270', 'carbohydrates': '16.00', 'fat': '13.00', 'protein': '21.00'}\n",
      "{'food_id': '1565707', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '360', 'carbohydrates': '9.00', 'fat': '13.00', 'protein': '48.00'}\n",
      "{'food_id': '855710', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 piece', 'calories': '240', 'carbohydrates': '10.00', 'fat': '11.00', 'protein': '25.00'}\n",
      "{'food_id': '6926210', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '5 oz', 'calories': '240', 'carbohydrates': '5.00', 'fat': '14.00', 'protein': '23.00'}\n",
      "{'food_id': '50672307', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 tray', 'calories': '430', 'carbohydrates': '11.00', 'fat': '24.00', 'protein': '43.00'}\n",
      "{'food_id': '226469', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 breast', 'calories': '580', 'carbohydrates': '21.00', 'fat': '25.00', 'protein': '68.00'}\n",
      "{'food_id': '54876480', 'food_name': 'Chicken Cordon Bleu', 'food_type': 'Brand', 'serving_description': '1 tray', 'calories': '670', 'carbohydrates': '29.00', 'fat': '43.00', 'protein': '46.00'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load data from the JSON file\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# Preprocess the data\n",
    "\n",
    "food_items = []\n",
    "for item in data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    food_type = item['food']['food_type']\n",
    "    servings = item['food']['servings']['serving']\n",
    "\n",
    "    # Process each serving entry\n",
    "    for serving in servings:\n",
    "        serving_description = serving['serving_description']\n",
    "        calories = serving['calories']\n",
    "        carbohydrates = serving['carbohydrate']\n",
    "        fat = serving['fat']\n",
    "        protein = serving['protein']\n",
    "\n",
    "        # Add the processed data to the food_items list\n",
    "        food_items.append({\n",
    "            'food_id': food_id,\n",
    "            'food_name': food_name,\n",
    "            'food_type': food_type,\n",
    "            'serving_description': serving_description,\n",
    "            'calories': calories,\n",
    "            'carbohydrates': carbohydrates,\n",
    "            'fat': fat,\n",
    "            'protein': protein\n",
    "        })\n",
    "# Print the first few food items\n",
    "for item in food_items[:20]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:103\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:487\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    484\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m    485\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    488\u001b[0m     element,\n\u001b[0;32m    489\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [Ellipsis] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m train_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((train_user_ids, train_item_ids, train_targets))\n\u001b[0;32m     68\u001b[0m val_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((val_user_ids, val_item_ids, val_targets))\n\u001b[1;32m---> 69\u001b[0m test_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices((test_user_ids, test_item_ids, test_targets))\n\u001b[0;32m     71\u001b[0m \u001b[39m# Shuffle and batch the datasets\u001b[39;00m\n\u001b[0;32m     72\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4708\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4706\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   4707\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4708\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[0;32m   4709\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4710\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:108\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 108\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, sparse_tensor\u001b[39m.\u001b[39mSparseTensorSpec):\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1629\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1631\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1635\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m   1637\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1638\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1641\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Load data from the food.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "        food_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Load user preferences from the user.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset.json', 'r') as file:\n",
    "        user_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Preprocess the food data\n",
    "food_items = []\n",
    "for item in food_data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    # Add other relevant features from the food.json file to the food_items list\n",
    "    food_items.append((food_id, food_name))\n",
    "\n",
    "# Preprocess the user preferences\n",
    "if isinstance(user_data, list):\n",
    "    user_preferences = user_data[0]['listUserPreferences']\n",
    "else:\n",
    "    user_preferences = user_data['listUserPreferences']\n",
    "\n",
    "gender = user_preferences['gender']\n",
    "weight = float(user_preferences['weight'])\n",
    "# Add other relevant user preferences from the user.json file\n",
    "\n",
    "# Define the variables\n",
    "user_vocab_size = 1000\n",
    "item_vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "\n",
    "# user_ids = [...]  # List of user IDs\n",
    "# Open the user_ids.json file and load its contents into the user_ids array\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_ids.json', 'r') as f:\n",
    "    user_ids = json.load(f)\n",
    "\n",
    "\n",
    "# item_ids = [...]  # List of item IDs\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/food_ids.json', 'r') as f:\n",
    "    food_id = json.load(f)\n",
    "\n",
    "\n",
    "# target_values = [...]  # List of target values\n",
    "target_values = []\n",
    "\n",
    "for user_id, item_id in zip(user_ids, food_id):\n",
    "    # Assuming user_data is a dictionary with user preferences\n",
    "    user_preferences = user_data[user_id]\n",
    "    # Function to get harmful diseases for a food item\n",
    "    harmful_diseases = get_harmful_diseases(item_id)\n",
    "\n",
    "    if any(disease in user_preferences['disease'] for disease in harmful_diseases):\n",
    "        # Assign lower target value for harmful foods\n",
    "        target_values.append(0.0)\n",
    "    else:\n",
    "        # Assign higher target value for preferred foods\n",
    "        target_values.append(1.0)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_size = int(0.7 * len(user_ids))\n",
    "val_size = int(0.15 * len(user_ids))\n",
    "test_size = len(user_ids) - train_size - val_size\n",
    "\n",
    "train_user_ids = user_ids[:train_size]\n",
    "train_item_ids = item_ids[:train_size]\n",
    "train_targets = target_values[:train_size]\n",
    "\n",
    "val_user_ids = user_ids[train_size:train_size+val_size]\n",
    "val_item_ids = item_ids[train_size:train_size+val_size]\n",
    "val_targets = target_values[train_size:train_size+val_size]\n",
    "\n",
    "test_user_ids = user_ids[train_size+val_size:]\n",
    "test_item_ids = item_ids[train_size+val_size:]\n",
    "test_targets = target_values[train_size+val_size:]\n",
    "\n",
    "# Convert the data to TensorFlow Datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_user_ids, train_item_ids, train_targets))\n",
    "val_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_user_ids, val_item_ids, val_targets))\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_user_ids, test_item_ids, test_targets))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "batch_size = 32\n",
    "train_data = train_data.shuffle(buffer_size=train_size).batch(batch_size)\n",
    "val_data = val_data.shuffle(buffer_size=val_size).batch(batch_size)\n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "# Define the collaborative filtering model\n",
    "\n",
    "\n",
    "class CollaborativeFilteringModel(tf.keras.Model):\n",
    "    def __init__(self, user_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Define the layers for collaborative filtering\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            user_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.embedding(inputs['user_id'])\n",
    "        return user_embeddings\n",
    "\n",
    "# Define the content-based model\n",
    "\n",
    "\n",
    "class ContentBasedModel(tf.keras.Model):\n",
    "    def __init__(self, item_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Define the layers for content-based filtering\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            item_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        item_embeddings = self.embedding(inputs['item_id'])\n",
    "        return item_embeddings\n",
    "\n",
    "# Define the hybrid model\n",
    "\n",
    "\n",
    "class HybridModel(tfrs.Model):\n",
    "    def __init__(self, user_vocab_size, item_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.collaborative_model = CollaborativeFilteringModel(\n",
    "            user_vocab_size, embedding_dim)\n",
    "        self.content_based_model = ContentBasedModel(\n",
    "            item_vocab_size, embedding_dim)\n",
    "        self.dense_layer = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.final_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.collaborative_model(inputs)\n",
    "        item_embeddings = self.content_based_model(inputs)\n",
    "        concatenated_embeddings = tf.concat(\n",
    "            [user_embeddings, item_embeddings], axis=1)\n",
    "        x = self.dense_layer(concatenated_embeddings)\n",
    "        output = self.final_layer(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Create an instance of the hybrid model\n",
    "model = HybridModel(user_vocab_size, item_vocab_size, embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001), loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_data)\n",
    "print(f\"Test RMSE: {results[1]}\")\n",
    "\n",
    "# Make recommendations\n",
    "# User ID for which recommendations will be generated\n",
    "user_inputs = {'user_id': tf.constant([user_ids])}\n",
    "# Item IDs to consider for recommendations\n",
    "item_inputs = {'item_id': tf.constant(item_ids)}\n",
    "recommendations = model.predict((user_inputs, item_inputs))\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the food.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "        food_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Load user preferences from the user.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset_new.json', 'r') as file:\n",
    "        user_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get harmful diseases for a given food item\n",
    "def get_harmful_diseases(food_id):\n",
    "    harmful_diseases = []\n",
    "    for item in food_data:\n",
    "        if item['food']['food_id'] == food_id:\n",
    "            harmful_diseases = item['food']['harmful_diseases']\n",
    "            break\n",
    "    return harmful_diseases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the food data\n",
    "food_items = []\n",
    "for item in food_data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    # Add other relevant features from the food.json file to the food_items list\n",
    "    food_items.append((food_id, food_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the user preferences\n",
    "if isinstance(user_data, list):\n",
    "    user_data = user_data[0]\n",
    "user_preferences = user_data['listUserPreferences']\n",
    "gender = user_preferences['gender']\n",
    "weight = float(user_preferences['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables\n",
    "user_vocab_size = 1000\n",
    "item_vocab_size = 1000\n",
    "embedding_dim = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids = [...]  # List of user IDs\n",
    "# Open the user_ids.json file and load its contents into the user_ids array\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_ids.json', 'r') as f:\n",
    "    user_ids = json.load(f)\n",
    "\n",
    "# item_ids = [...]  # List of item IDs\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/food_ids.json', 'r') as f:\n",
    "    item_ids = json.load(f)['food_ids']\n",
    "\n",
    "target_values = []  # List of target values\n",
    "\n",
    "# Fill target values based on user preferences and harmful diseases\n",
    "for user_id, item_id in zip(user_ids, item_ids):\n",
    "    user_preference = user_data.get(str(user_id))\n",
    "    if user_preference is None:\n",
    "        # User preferences not found, assign lower target value\n",
    "        target_values.append(0.0)\n",
    "    else:\n",
    "        user_preference = user_preference['listUserPreferences']\n",
    "        harmful_diseases = get_harmful_diseases(item_id)\n",
    "\n",
    "        if any(disease in user_preference['disease'] for disease in harmful_diseases):\n",
    "            # Assign lower target value for harmful foods\n",
    "            target_values.append(0.0)\n",
    "        else:\n",
    "            # Assign higher target value for preferred foods\n",
    "            target_values.append(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_size = int(0.7 * len(user_ids))\n",
    "val_size = int(0.15 * len(user_ids))\n",
    "test_size = len(user_ids) - train_size - val_size\n",
    "\n",
    "train_user_ids = user_ids[:train_size]\n",
    "train_item_ids = item_ids[:train_size]\n",
    "train_targets = target_values[:train_size]\n",
    "\n",
    "val_user_ids = user_ids[train_size:train_size+val_size]\n",
    "val_item_ids = item_ids[train_size:train_size+val_size]\n",
    "val_targets = target_values[train_size:train_size+val_size]\n",
    "\n",
    "test_user_ids = user_ids[train_size+val_size:]\n",
    "test_item_ids = item_ids[train_size+val_size:]\n",
    "test_targets = target_values[train_size+val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_user_ids size: 350\n",
      "train_item_ids size: 265\n",
      "train_targets size: 265\n",
      "val_user_ids size: 75\n",
      "val_item_ids size: 0\n",
      "val_targets size: 0\n",
      "test_user_ids size: 75\n",
      "test_item_ids size: 0\n",
      "test_targets size: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"train_user_ids size:\", len(train_user_ids))\n",
    "print(\"train_item_ids size:\", len(train_item_ids))\n",
    "print(\"train_targets size:\", len(train_targets))\n",
    "\n",
    "print(\"val_user_ids size:\", len(val_user_ids))\n",
    "print(\"val_item_ids size:\", len(val_item_ids))\n",
    "print(\"val_targets size:\", len(val_targets))\n",
    "\n",
    "print(\"test_user_ids size:\", len(test_user_ids))\n",
    "print(\"test_item_ids size:\", len(test_item_ids))\n",
    "print(\"test_targets size:\", len(test_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 265 and 350 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Convert the data to TensorFlow Datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices(\n\u001b[0;32m      3\u001b[0m     ({\u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_user_ids, \u001b[39m\"\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_item_ids}, train_targets)\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m val_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m      6\u001b[0m     ({\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: val_user_ids, \u001b[39m\"\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m\"\u001b[39m: val_item_ids}, val_targets)\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m test_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m      9\u001b[0m     ({\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: test_user_ids, \u001b[39m\"\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m\"\u001b[39m: test_item_ids}, test_targets)\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4720\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4717\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[0;32m   4718\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n\u001b[0;32m   4719\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m-> 4720\u001b[0m   batch_dim\u001b[39m.\u001b[39;49massert_is_compatible_with(\n\u001b[0;32m   4721\u001b[0m       tensor_shape\u001b[39m.\u001b[39;49mDimension(\n\u001b[0;32m   4722\u001b[0m           tensor_shape\u001b[39m.\u001b[39;49mdimension_value(t\u001b[39m.\u001b[39;49mget_shape()[\u001b[39m0\u001b[39;49m])))\n\u001b[0;32m   4724\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mtensor_slice_dataset(\n\u001b[0;32m   4725\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors,\n\u001b[0;32m   4726\u001b[0m     output_shapes\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_shapes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure),\n\u001b[0;32m   4727\u001b[0m     is_files\u001b[39m=\u001b[39mis_files,\n\u001b[0;32m   4728\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mSerializeToString())\n\u001b[0;32m   4729\u001b[0m \u001b[39msuper\u001b[39m(TensorSliceDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:299\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m    is_compatible_with).\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m--> 299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDimensions \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m are not compatible\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    300\u001b[0m                    (\u001b[39mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions 265 and 350 are not compatible"
     ]
    }
   ],
   "source": [
    "# Convert the data to TensorFlow Datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": train_user_ids, \"item_id\": train_item_ids}, train_targets)\n",
    ")\n",
    "val_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": val_user_ids, \"item_id\": val_item_ids}, val_targets)\n",
    ")\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": test_user_ids, \"item_id\": test_item_ids}, test_targets)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            user_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.user_embedding(inputs[\"user_id\"])\n",
    "\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.item_embedding = tf.keras.layers.Embedding(\n",
    "            item_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.item_embedding(inputs[\"item_id\"])\n",
    "\n",
    "\n",
    "class RecommenderModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_model = UserModel()\n",
    "        self.item_model = ItemModel()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features)\n",
    "        item_embeddings = self.item_model(features)\n",
    "        return self.task(user_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m recommender_model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdagrad(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m recommender_model\u001b[39m.\u001b[39mfit(train_data\u001b[39m.\u001b[39mshuffle(\u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39mbatch(\u001b[39m32\u001b[39m), epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m eval_results \u001b[39m=\u001b[39m recommender_model\u001b[39m.\u001b[39mevaluate(val_data\u001b[39m.\u001b[39mbatch(\u001b[39m32\u001b[39m), return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an instance of the RecommenderModel\n",
    "recommender_model = RecommenderModel()\n",
    "\n",
    "# Compile the model\n",
    "recommender_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Train the model\n",
    "recommender_model.fit(train_data.shuffle(1000).batch(32), epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = recommender_model.evaluate(val_data.batch(32), return_dict=True)\n",
    "print(f\"Validation RMSE: {eval_results['root_mean_squared_error']}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = recommender_model.predict(test_data.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m target_values \u001b[39m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m user_id, item_id \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(user_ids, food_ids):\n\u001b[1;32m---> 55\u001b[0m     user_preferences \u001b[39m=\u001b[39m user_data[user_id][\u001b[39m'\u001b[39m\u001b[39mlistUserPreferences\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m     harmful_diseases \u001b[39m=\u001b[39m get_harmful_diseases(item_id)\n\u001b[0;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(disease \u001b[39min\u001b[39;00m user_preferences[\u001b[39m'\u001b[39m\u001b[39mdisease\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m disease \u001b[39min\u001b[39;00m harmful_diseases):\n\u001b[0;32m     59\u001b[0m         \u001b[39m# Assign lower target value for harmful foods\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Load data from the food.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "        food_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Load user preferences from the user.json file\n",
    "try:\n",
    "    with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset.json', 'r') as file:\n",
    "        user_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "    exit()\n",
    "\n",
    "# Preprocess the food data\n",
    "food_items = []\n",
    "for item in food_data:\n",
    "    food_id = item['food']['food_id']\n",
    "    food_name = item['food']['food_name']\n",
    "    # Add other relevant features from the food.json file to the food_items list\n",
    "    food_items.append((food_id, food_name))\n",
    "\n",
    "# Preprocess the user preferences\n",
    "if isinstance(user_data, list):\n",
    "    user_preferences = user_data[0]['listUserPreferences']\n",
    "else:\n",
    "    user_preferences = user_data['listUserPreferences']\n",
    "\n",
    "gender = user_preferences['gender']\n",
    "weight = float(user_preferences['weight'])\n",
    "# Add other relevant user preferences from the user.json file\n",
    "\n",
    "# Define the variables\n",
    "user_vocab_size = 1000\n",
    "item_vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "\n",
    "# Load user and item IDs from files\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_ids.json', 'r') as f:\n",
    "    user_ids = json.load(f)\n",
    "\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/food_ids.json', 'r') as f:\n",
    "    food_ids = json.load(f)\n",
    "\n",
    "# Define target values based on user preferences and harmful diseases\n",
    "target_values = []\n",
    "for user_id, item_id in zip(user_ids, food_ids):\n",
    "    user_preferences = user_data[user_id]['listUserPreferences']\n",
    "    harmful_diseases = get_harmful_diseases(item_id)\n",
    "\n",
    "    if any(disease in user_preferences['disease'] for disease in harmful_diseases):\n",
    "        # Assign lower target value for harmful foods\n",
    "        target_values.append(0.0)\n",
    "    else:\n",
    "        # Assign higher target value for preferred foods\n",
    "        target_values.append(1.0)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_size = int(0.7 * len(user_ids))\n",
    "val_size = int(0.15 * len(user_ids))\n",
    "test_size = len(user_ids) - train_size - val_size\n",
    "\n",
    "train_user_ids = user_ids[:train_size]\n",
    "train_item_ids = food_ids[:train_size]\n",
    "train_targets = target_values[:train_size]\n",
    "\n",
    "val_user_ids = user_ids[train_size:train_size+val_size]\n",
    "val_item_ids = food_ids[train_size:train_size+val_size]\n",
    "val_targets = target_values[train_size:train_size+val_size]\n",
    "\n",
    "test_user_ids = user_ids[train_size+val_size:]\n",
    "test_item_ids = food_ids[train_size+val_size:]\n",
    "test_targets = target_values[train_size+val_size:]\n",
    "\n",
    "# Convert the data to TensorFlow Datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": train_user_ids, \"item_id\": train_item_ids}, train_targets))\n",
    "val_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": val_user_ids, \"item_id\": val_item_ids}, val_targets))\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_id\": test_user_ids, \"item_id\": test_item_ids}, test_targets))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "batch_size = 32\n",
    "train_data = train_data.shuffle(buffer_size=train_size).batch(batch_size)\n",
    "val_data = val_data.shuffle(buffer_size=val_size).batch(batch_size)\n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "# Define the collaborative filtering model\n",
    "\n",
    "\n",
    "class CollaborativeFilteringModel(tf.keras.Model):\n",
    "    def __init__(self, user_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            user_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.embedding(inputs['user_id'])\n",
    "        return user_embeddings\n",
    "\n",
    "# Define the content-based model\n",
    "\n",
    "\n",
    "class ContentBasedModel(tf.keras.Model):\n",
    "    def __init__(self, item_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            item_vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        item_embeddings = self.embedding(inputs['item_id'])\n",
    "        return item_embeddings\n",
    "\n",
    "# Define the hybrid model\n",
    "\n",
    "\n",
    "class HybridModel(tfrs.Model):\n",
    "    def __init__(self, user_vocab_size, item_vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.collaborative_model = CollaborativeFilteringModel(\n",
    "            user_vocab_size, embedding_dim)\n",
    "        self.content_based_model = ContentBasedModel(\n",
    "            item_vocab_size, embedding_dim)\n",
    "        self.dense_layer = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.final_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embeddings = self.collaborative_model(inputs)\n",
    "        item_embeddings = self.content_based_model(inputs)\n",
    "        concatenated_embeddings = tf.concat(\n",
    "            [user_embeddings, item_embeddings], axis=1)\n",
    "        x = self.dense_layer(concatenated_embeddings)\n",
    "        output = self.final_layer(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of the hybrid model\n",
    "model = HybridModel(user_vocab_size, item_vocab_size, embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_data)\n",
    "print(f\"Test RMSE: {results[1]}\")\n",
    "\n",
    "# Make recommendations\n",
    "# User IDs for which recommendations will be generated\n",
    "user_inputs = {'user_id': np.array(user_ids)}\n",
    "# Item IDs to consider for recommendations\n",
    "item_inputs = {'item_id': np.array(food_ids)}\n",
    "recommendations = model.predict((user_inputs, item_inputs))\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\ipykernel_5388\\4247999888.py\", line 75, in compute_loss\n        return self.task(user_embeddings, food_embeddings)\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py\", line 159, in tf__call\n        ag__.if_stmt(ag__.ld(compute_metrics), if_body_5, else_body_5, get_state_7, set_state_7, (), 0)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py\", line 155, in if_body_5\n        ag__.for_stmt(ag__.ld(self)._factorized_metrics, None, loop_body_1, get_state_6, set_state_6, (), {'iterate_names': 'metric'})\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py\", line 154, in loop_body_1\n        ag__.converted_call(ag__.ld(update_ops).append, (ag__.converted_call(ag__.ld(metric).update_state, (ag__.ld(query_embeddings), ag__.ld(candidate_embeddings)[:ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(query_embeddings),), None, fscope)[0]]), dict(true_candidate_ids=ag__.ld(candidate_ids)), fscope),), None, fscope)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_files5lngy8y.py\", line 48, in tf__update_state\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(true_candidate_ids) is None, lambda : ag__.not_(ag__.converted_call(ag__.ld(self)._candidates.is_exact, (), None, fscope))), if_body, else_body, get_state, set_state, (), 0)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_files5lngy8y.py\", line 48, in <lambda>\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(true_candidate_ids) is None, lambda : ag__.not_(ag__.converted_call(ag__.ld(self)._candidates.is_exact, (), None, fscope))), if_body, else_body, get_state, set_state, (), 0)\n\n    AttributeError: Exception encountered when calling layer \"retrieval_1\" \"                 f\"(type Retrieval).\n    \n    in user code:\n    \n        File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\tasks\\retrieval.py\", line 197, in call  *\n            update_ops.append(\n        File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\metrics\\factorized_top_k.py\", line 125, in update_state  *\n            if true_candidate_ids is None and not self._candidates.is_exact():\n    \n        AttributeError: 'FoodModel' object has no attribute 'is_exact'\n    \n    \n    Call arguments received by layer \"retrieval_1\" \"                 f\"(type Retrieval):\n      • query_embeddings=tf.Tensor(shape=(None, 32), dtype=float32)\n      • candidate_embeddings=tf.Tensor(shape=(None, 32), dtype=float32)\n      • sample_weight=None\n      • candidate_sampling_probability=None\n      • candidate_ids=None\n      • compute_metrics=True\n      • compute_batch_metrics=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 126\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[39myield\u001b[39;00m {\n\u001b[0;32m    116\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: user_ids\u001b[39m.\u001b[39mindex(user_id),\n\u001b[0;32m    117\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mfood_id\u001b[39m\u001b[39m\"\u001b[39m: food_ids\u001b[39m.\u001b[39mindex(random_food_id)\n\u001b[0;32m    118\u001b[0m                     }\n\u001b[0;32m    121\u001b[0m train_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_generator(prepare_data, output_signature\u001b[39m=\u001b[39m(\n\u001b[0;32m    122\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m: tf\u001b[39m.\u001b[39mTensorSpec(shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32),\n\u001b[0;32m    123\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mfood_id\u001b[39m\u001b[39m\"\u001b[39m: tf\u001b[39m.\u001b[39mTensorSpec(shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)}\n\u001b[0;32m    124\u001b[0m ))\n\u001b[1;32m--> 126\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_data\u001b[39m.\u001b[39;49mbatch(\u001b[39m4096\u001b[39;49m), epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedtirejz7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py:68\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> 68\u001b[0m   loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(inputs, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     70\u001b[0m   \u001b[39m# Handle regularization losses as well.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m   regularization_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n",
      "Cell \u001b[1;32mIn[9], line 75\u001b[0m, in \u001b[0;36mRecommenderModel.compute_loss\u001b[1;34m(self, features, training)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m# Concatenate positive and negative examples\u001b[39;00m\n\u001b[0;32m     72\u001b[0m food_embeddings \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat(\n\u001b[0;32m     73\u001b[0m     [positive_food_embeddings, negative_food_embeddings], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask(user_embeddings, food_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py:159\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, query_embeddings, candidate_embeddings, sample_weight, candidate_sampling_probability, candidate_ids, compute_metrics, compute_batch_metrics)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body_5\u001b[39m():\n\u001b[0;32m    158\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(compute_metrics), if_body_5, else_body_5, get_state_7, set_state_7, (), \u001b[39m0\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_9\u001b[39m():\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py:155\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_5\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m     metric \u001b[39m=\u001b[39m itr_1\n\u001b[0;32m    154\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(update_ops)\u001b[39m.\u001b[39mappend, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(metric)\u001b[39m.\u001b[39mupdate_state, (ag__\u001b[39m.\u001b[39mld(query_embeddings), ag__\u001b[39m.\u001b[39mld(candidate_embeddings)[:ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mshape, (ag__\u001b[39m.\u001b[39mld(query_embeddings),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m0\u001b[39m]]), \u001b[39mdict\u001b[39m(true_candidate_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(candidate_ids)), fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m--> 155\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_factorized_metrics, \u001b[39mNone\u001b[39;49;00m, loop_body_1, get_state_6, set_state_6, (), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmetric\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py:154\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_5.<locals>.loop_body_1\u001b[1;34m(itr_1)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloop_body_1\u001b[39m(itr_1):\n\u001b[0;32m    153\u001b[0m     metric \u001b[39m=\u001b[39m itr_1\n\u001b[1;32m--> 154\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(update_ops)\u001b[39m.\u001b[39mappend, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(metric)\u001b[39m.\u001b[39;49mupdate_state, (ag__\u001b[39m.\u001b[39;49mld(query_embeddings), ag__\u001b[39m.\u001b[39;49mld(candidate_embeddings)[:ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mshape, (ag__\u001b[39m.\u001b[39;49mld(query_embeddings),), \u001b[39mNone\u001b[39;49;00m, fscope)[\u001b[39m0\u001b[39;49m]]), \u001b[39mdict\u001b[39;49m(true_candidate_ids\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(candidate_ids)), fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_files5lngy8y.py:48\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state\u001b[1;34m(self, query_embeddings, true_candidate_embeddings, true_candidate_ids, sample_weight)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body\u001b[39m():\n\u001b[0;32m     47\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(true_candidate_ids) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_candidates\u001b[39m.\u001b[39mis_exact, (), \u001b[39mNone\u001b[39;00m, fscope))), if_body, else_body, get_state, set_state, (), \u001b[39m0\u001b[39m)\n\u001b[0;32m     49\u001b[0m positive_scores \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_sum, (ag__\u001b[39m.\u001b[39mld(query_embeddings) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(true_candidate_embeddings),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     50\u001b[0m (top_k_predictions, retrieved_ids) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_candidates, (ag__\u001b[39m.\u001b[39mld(query_embeddings),), \u001b[39mdict\u001b[39m(k\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mmax\u001b[39m), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_ks,), \u001b[39mNone\u001b[39;00m, fscope)), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_files5lngy8y.py:48\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body\u001b[39m():\n\u001b[0;32m     47\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(true_candidate_ids) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_candidates\u001b[39m.\u001b[39;49mis_exact, (), \u001b[39mNone\u001b[39;00m, fscope))), if_body, else_body, get_state, set_state, (), \u001b[39m0\u001b[39m)\n\u001b[0;32m     49\u001b[0m positive_scores \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_sum, (ag__\u001b[39m.\u001b[39mld(query_embeddings) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(true_candidate_embeddings),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     50\u001b[0m (top_k_predictions, retrieved_ids) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_candidates, (ag__\u001b[39m.\u001b[39mld(query_embeddings),), \u001b[39mdict\u001b[39m(k\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mmax\u001b[39m), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_ks,), \u001b[39mNone\u001b[39;00m, fscope)), fscope)\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\ipykernel_5388\\4247999888.py\", line 75, in compute_loss\n        return self.task(user_embeddings, food_embeddings)\n    File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py\", line 159, in tf__call\n        ag__.if_stmt(ag__.ld(compute_metrics), if_body_5, else_body_5, get_state_7, set_state_7, (), 0)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py\", line 155, in if_body_5\n        ag__.for_stmt(ag__.ld(self)._factorized_metrics, None, loop_body_1, get_state_6, set_state_6, (), {'iterate_names': 'metric'})\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_filer5o89w8h.py\", line 154, in loop_body_1\n        ag__.converted_call(ag__.ld(update_ops).append, (ag__.converted_call(ag__.ld(metric).update_state, (ag__.ld(query_embeddings), ag__.ld(candidate_embeddings)[:ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(query_embeddings),), None, fscope)[0]]), dict(true_candidate_ids=ag__.ld(candidate_ids)), fscope),), None, fscope)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_files5lngy8y.py\", line 48, in tf__update_state\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(true_candidate_ids) is None, lambda : ag__.not_(ag__.converted_call(ag__.ld(self)._candidates.is_exact, (), None, fscope))), if_body, else_body, get_state, set_state, (), 0)\n    File \"C:\\Users\\Ananda\\AppData\\Local\\Temp\\__autograph_generated_files5lngy8y.py\", line 48, in <lambda>\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(true_candidate_ids) is None, lambda : ag__.not_(ag__.converted_call(ag__.ld(self)._candidates.is_exact, (), None, fscope))), if_body, else_body, get_state, set_state, (), 0)\n\n    AttributeError: Exception encountered when calling layer \"retrieval_1\" \"                 f\"(type Retrieval).\n    \n    in user code:\n    \n        File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\tasks\\retrieval.py\", line 197, in call  *\n            update_ops.append(\n        File \"c:\\Users\\Ananda\\anaconda3\\lib\\site-packages\\tensorflow_recommenders\\metrics\\factorized_top_k.py\", line 125, in update_state  *\n            if true_candidate_ids is None and not self._candidates.is_exact():\n    \n        AttributeError: 'FoodModel' object has no attribute 'is_exact'\n    \n    \n    Call arguments received by layer \"retrieval_1\" \"                 f\"(type Retrieval):\n      • query_embeddings=tf.Tensor(shape=(None, 32), dtype=float32)\n      • candidate_embeddings=tf.Tensor(shape=(None, 32), dtype=float32)\n      • sample_weight=None\n      • candidate_sampling_probability=None\n      • candidate_ids=None\n      • compute_metrics=True\n      • compute_batch_metrics=True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import random\n",
    "\n",
    "# Load the datasets\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_dataset.json', 'r') as file:\n",
    "    user_data = json.load(file)  # Load user.json dataset\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/output.json', 'r') as file:\n",
    "    food_data = json.load(file)  # Load food.json dataset\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/user_ids.json', 'r') as f:\n",
    "    user_ids = json.load(f)  # Load user_ids.json dataset\n",
    "with open('C:/Users/Ananda/OneDrive/repos/NutriPal-ML/results/food_ids.json', 'r') as f:\n",
    "    food_ids = json.load(f)  # Load food_ids.json dataset\n",
    "\n",
    "# Define the model inputs and features\n",
    "\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the user features\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            len(user_ids), output_dim=32\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_embedding = self.user_embedding(inputs[\"user_id\"])\n",
    "        return user_embedding\n",
    "\n",
    "\n",
    "class FoodModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the food features\n",
    "        self.food_embedding = tf.keras.layers.Embedding(\n",
    "            len(food_ids), output_dim=32\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        food_embedding = self.food_embedding(inputs[\"food_id\"])\n",
    "        return food_embedding\n",
    "\n",
    "# Define the loss and metrics\n",
    "\n",
    "\n",
    "class RecommenderModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the user and food models\n",
    "        self.user_model = UserModel()\n",
    "        self.food_model = FoodModel()\n",
    "\n",
    "        # Define the retrieval task\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=self.food_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features)\n",
    "        positive_food_embeddings = self.food_model(features)\n",
    "\n",
    "        # Sample negative food examples\n",
    "        negative_food_ids = tf.random.shuffle(food_ids_tensor)[:100]\n",
    "        negative_food_embeddings = self.food_model(\n",
    "            {\"food_id\": negative_food_ids})\n",
    "\n",
    "        # Concatenate positive and negative examples\n",
    "        food_embeddings = tf.concat(\n",
    "            [positive_food_embeddings, negative_food_embeddings], axis=0)\n",
    "\n",
    "        return self.task(user_embeddings, food_embeddings)\n",
    "\n",
    "\n",
    "# Convert food_ids to TensorFlow tensor\n",
    "food_ids_tensor = tf.constant(food_ids)\n",
    "\n",
    "# Instantiate the model and compile it\n",
    "model = RecommenderModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Prepare the data for training\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    for user in user_data:\n",
    "        user_id = user[\"id_user\"]\n",
    "        user_food_preferences = user[\"listUserPreferences\"][\"favoriteFood\"]\n",
    "        user_diseases = user[\"listUserPreferences\"][\"disease\"]\n",
    "\n",
    "        for food in food_data:\n",
    "            food_id = food[\"food\"][\"food_id\"]\n",
    "            food_name = food[\"food\"][\"food_name\"]\n",
    "\n",
    "            # Filter foods based on user's disease\n",
    "            is_safe_food = True\n",
    "            for disease in user_diseases:\n",
    "                if disease in food_name.lower():\n",
    "                    is_safe_food = False\n",
    "                    break\n",
    "\n",
    "            if is_safe_food:\n",
    "                # Create positive example (user, food)\n",
    "                yield {\n",
    "                    \"user_id\": user_ids.index(user_id),\n",
    "                    \"food_id\": food_ids.index(food_id)\n",
    "                }\n",
    "\n",
    "                # Create negative examples by randomly sampling foods\n",
    "                for _ in range(5):\n",
    "                    random_food_id = random.choice(food_ids)\n",
    "                    yield {\n",
    "                        \"user_id\": user_ids.index(user_id),\n",
    "                        \"food_id\": food_ids.index(random_food_id)\n",
    "                    }\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_generator(prepare_data, output_signature=(\n",
    "    {\"user_id\": tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "     \"food_id\": tf.TensorSpec(shape=(), dtype=tf.int32)}\n",
    "))\n",
    "\n",
    "model.fit(train_data.batch(4096), epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2db04e191085c925f35704d5645a1f5ec8c10b267e24c755d6661bdf1429d0eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
